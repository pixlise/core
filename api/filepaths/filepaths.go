// Licensed to NASA JPL under one or more contributor
// license agreements. See the NOTICE file distributed with
// this work for additional information regarding copyright
// ownership. NASA JPL licenses this file to you under
// the Apache License, Version 2.0 (the "License"); you may
// not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

// Defines all paths/file names used in S3 for storage of our data. This allows a more centralised
// view of where our data is in S3 and makes changing storage/paths easier
package filepaths

import (
	"path"
)

// This package contains all file paths that the PIXLISE API should ever need to access. All been centralised
// so we have one place to change it all. Several helper functions are provided to make certain paths given
// a user ID or dataset ID, etc.

// Our data resides across several buckets:

////////////////////////////////////////////////////////////////////////////////////
// Data Bucket
////////////////////////////////////////////////////////////////////////////////////

// Paths for v4 API:
const DatasetImagesRoot = "Images"
const DatasetImageCacheRoot = "Image-Cache"
const DatasetScansRoot = "Scans"

func GetScanFilePath(scanID string, fileName string) string {
	return path.Join(DatasetScansRoot, scanID, fileName)
}

func GetImageFilePath(imagePath string) string {
	return path.Join(DatasetImagesRoot, imagePath)
}

func GetImageCacheFilePath(imagePath string) string {
	return path.Join(DatasetImageCacheRoot, imagePath)
}

/*
Root directory containing our dataset files
  - Datasets/
    ----<dataset-id>/
    --------dataset.bin
    --------Context image files (.png or .jpg)
    --------RGBU multi-spectral files (.tif)
    --------diffraction-db.bin
    --------summary.json
*/

// The dataset file containing all spectra, housekeeping, beam locations, etc. Created by data-converter
const DatasetFileName = "dataset.bin"

// Diffraction peak database, generated by diffraction-detector when dataset is imported
const DiffractionDBFileName = "diffraction-db.bin"

/*
Root directory containing all archived data set zips as we downloaded them
  - Archive/
*/
const RootArchive = "Archive"

////////////////////////////////////////////////////////////////////////////////////
// Config Bucket
////////////////////////////////////////////////////////////////////////////////////

/*
Root directory containing all dataset configs
  - DatasetConfig/
  - ----import-times.json - Specifies when each dataset was imported (map id->unix time)
*/
const RootDatasetConfig = "DatasetConfig"

/*
Root directory containing all detector configs
  - DetectorConfig/
  - ----<config-name>/ - Name shown on UI, eg PIXL or Breadboard
  - --------pixlise-config.json - UI config values for this detector, eg detector energy range, window material, etc
  - --------PiquantConfigs/
  - ------------<version>/ - eg v1, v2, v3
  - ----------------config.json - The PIQUANT config file, used by quant "runner", in docker container. References other files
  - ----------------<other files>.msa or .csv - These are referenced by config.json and passed to PIQUANT exe as parameters
*/
const RootDetectorConfig = "DetectorConfig"

// Piquant configs sub-dir
//   - NOTE: Quant creation doesn't use GetDetectorConfigPath, maybe DetectorConfig is hard-coded into docker container!
//     TODO: remove that
const PiquantConfigSubDir = "PiquantConfigs"

// Get a detector config path (used by PIQUANT) given the config name, version and optionally a file name. If file name is blank
// then the directory path above it is returned
func GetDetectorConfigPath(configName string, version string, fileName string) string {
	if len(version) > 0 {
		if len(fileName) > 0 {
			return path.Join(RootDetectorConfig, configName, PiquantConfigSubDir, version, fileName)
		} else {
			return path.Join(RootDetectorConfig, configName, PiquantConfigSubDir, version)
		}
	}
	return path.Join(RootDetectorConfig, configName, PiquantConfigSubDir)
}

// File name of "overall" piquant config file, which references the individual files PIQUANT will need
const PiquantConfigFileName = "config.json"

/*
Root directory of PIXLISE-specific config files
  - PixliseConfig/
  - ----auth0.pem - Certificate needed by Auth0 to verify a user request is valid
  - ----datasets.json - Dataset list (tiles)
  - ----piquant-version.json - Docker container for running PIQUANT
  - ----bad-dataset-ids.json - Contains a list of Dataset IDs to ignore when generating dataset tiles
*/
const RootPixliseConfigPath = "PixliseConfig"

// Auth0 PEM file which API uses to verify JWTs
const Auth0PemFileName = "auth0.pem"

// Getting a config file path relative to the root of the bucket
func GetConfigFilePath(fileName string) string {
	return path.Join(RootPixliseConfigPath, fileName)
}

const RootUserContent = "UserContent"

const RootQuantificationPath = "Quantifications"

func GetQuantPath(userId string, scanId string, fileName string) string {
	if len(fileName) > 0 {
		return path.Join(RootQuantificationPath, scanId, userId, fileName)
	}
	return path.Join(RootQuantificationPath, scanId, userId)
}

// File name of last piquant output (used with fit command). Extension added as needed
const QuantLastOutputFileName = "output_data"

// File name of last piquant output log file (used with fit command)
const QuantLastOutputLogName = "output.log"

// Retrieve path for last outputs, eg last run fit command (command is actually "quant"), sits in here with its log file
const rootLastQuantOutput = "LastQuantOutput"

func GetUserLastPiquantOutputPath(userID string, datasetID string, piquantCommand string, fileName string) string {
	if len(fileName) > 0 {
		return path.Join(rootLastQuantOutput, datasetID, userID, piquantCommand, fileName)
	}
	return path.Join(rootLastQuantOutput, datasetID, userID, piquantCommand)
}

// Name of user manually entered diffraction peaks file. NOTE: this file only exists as a shared file!
const DiffractionPeakManualFileName = "manual-diffraction-peaks.json"

// Name of file containing status of diffraction peaks - the diffraction DB is generated when dataset is created
// but users can view a peak and mark it with a status, eg to delete it because it's not a real diffraction peak
// OTE: this file only exists as a shared file!
const DiffractionPeakStatusFileName = "diffraction-peak-statuses.json"

////////////////////////////////////////////////////////////////////////////////////
// Job Bucket
////////////////////////////////////////////////////////////////////////////////////

/*
This contains temporary files generated when running a long-running job (eg PIQUANT).
Contains parameters to the job, status files, log files from the job, intermediate calculation files
These are in separate directories to aid listing, so instead of returning 100s of files per job
you may only want a list of job statuses, where you'd only get 1 file per job
  - JobData/
  - ----<dataset-id>/
  - --------<job-id>/
  - ------------node*.pmcs - PMC list for a given node running the job
  - ------------params.json - Job parameters as specified when created
  - ------------output/
  - ----------------node*.pmcs_result.csv - CSV generated by a single node, intermediate outpu
  - ----------------combined.csv - The final output generated by combining all the node*.pmcs_result.csv files
  - ------------piquant-logs/
  - ----------------node*.pmcs_piquant.log - PIQUANT log file for a given node
  - ----------------node*.pmcs_stdout.log - stdout for running PIQUANT on a given node
*/
const RootJobData = "JobData"

// Piquant logs sub-directory
const PiquantLogSubdir = "piquant-logs"

// Retrieves the path of a given file for dataset, job id and file name. NOTE: if job ID is blank, it's omitted
// from the path, and same for file name
func GetJobDataPath(datasetID string, jobID string, fileName string) string {
	if len(jobID) > 0 {
		if len(fileName) > 0 {
			return path.Join(RootJobData, datasetID, jobID, fileName)
		}
		return path.Join(RootJobData, datasetID, jobID)
	}
	return path.Join(RootJobData, datasetID)
}

////////////////////////////////////////////////////////////////////////////////////
// Artifacts Manual Upload Data Source Bucket
////////////////////////////////////////////////////////////////////////////////////

/*
Root directory for all dataset add-ons. These are custom files that can be uploaded for a datset to set its
title, and which the "default" image is, etc.
  - dataset-addons/
  - ----<dataset-id>/
  - --------custom-meta.json - Custom metadata for this dataset, usually to set dataset title, but can also contain matched image scale/bias or other fields
  - ------------UNALINED/
  - ----------------image, *.png or *.jpg
  - ------------MATCHED/
  - ----------------image, *.png, *.jpg or *.tif (if TIF it's considered an RGBU multi-spectral image)
  - ------------RGBU/
  - ----------------images, *.tif - NOTE: Went unused, these are now all stored as MATCHED images
*/
const DatasetCustomRoot = "dataset-addons"

// File name for dataset custom meta file containing the title and other settings
const DatasetCustomMetaFileName = "custom-meta.json"

/*
Root directory to store uploaded dataset "raw" artifacts. These are then read by dataset importer
to create a dataset in the dataset bucket
  - Uploaded/
  - ----<dataset-id>/
  - --------Files for that dataset importer type. For example, with breadboards we expect:
  - --------import.json <-- Describes what's what
  - --------spectra.zip <-- Spectra .msa files zipped up
  - --------context_image_1.jpg <-- 1 or more context images
*/
const DatasetUploadRoot = "UploadedDatasets"

////////////////////////////////////////////////////////////////////////////////////
// Helpers for forming certain file names
////////////////////////////////////////////////////////////////////////////////////

// QuantFileSuffix - quant files are <jobid>.bin
const QuantFileSuffix = ".bin"

func MakeQuantDataFileName(quantID string) string {
	return quantID + QuantFileSuffix
}

// CSVFileSuffix - CSV files are <jobid>.csv
const CSVFileSuffix = ".csv"

func MakeQuantCSVFileName(quantID string) string {
	return quantID + CSVFileSuffix
}

// QuantLogsSubDirSuffix - this goes after job ID to form a directory name that stores the quant logs
const QuantLogsSubDirSuffix = "-logs"

func MakeQuantLogDirName(quantID string) string {
	return quantID + QuantLogsSubDirSuffix
}
